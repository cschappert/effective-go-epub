<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
    <title>Effective Go - The Go Programming Language</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <link href="stylesheet.css" rel="stylesheet" type="text/css" />
    <link href="page_styles.css" rel="stylesheet" type="text/css" />
</head>

<body class="site">
    <main id="page" class="site-content">
        <div class="container">
            <h2 id="concurrency" class="calibre3">Concurrency</h2>

            <h3 id="sharing" class="calibre5">Share by communicating</h3>

            <p class="calibre4">
                Concurrent programming is a large topic and there is space only for some
                Go-specific highlights here.
            </p>
            <p class="calibre4">
                Concurrent programming in many environments is made difficult by the
                subtleties required to implement correct access to shared variables. Go encourages
                a different approach in which shared values are passed around on channels
                and, in fact, never actively shared by separate threads of execution.
                Only one goroutine has access to the value at any given time.
                Data races cannot occur, by design.
                To encourage this way of thinking we have reduced it to a slogan:
            </p>
            <blockquote class="calibre16">
                Do not communicate by sharing memory;
                instead, share memory by communicating.
            </blockquote>
            <p class="calibre4">
                This approach can be taken too far. Reference counts may be best done
                by putting a mutex around an integer variable, for instance. But as a
                high-level approach, using channels to control access makes it easier
                to write clear, correct programs.
            </p>
            <p class="calibre4">
                One way to think about this model is to consider a typical single-threaded
                program running on one CPU. It has no need for synchronization primitives.
                Now run another such instance; it too needs no synchronization. Now let those
                two communicate; if the communication is the synchronizer, there's still no need
                for other synchronization. Unix pipelines, for example, fit this model
                perfectly. Although Go's approach to concurrency originates in Hoare's
                Communicating Sequential Processes (CSP),
                it can also be seen as a type-safe generalization of Unix pipes.
            </p>

            <h3 id="goroutines" class="calibre5">Goroutines</h3>

            <p class="calibre4">
                They're called <em class="calibre12">goroutines</em> because the existing
                terms—threads, coroutines, processes, and so on—convey
                inaccurate connotations. A goroutine has a simple model: it is a
                function executing concurrently with other goroutines in the same
                address space. It is lightweight, costing little more than the
                allocation of stack space.
                And the stacks start small, so they are cheap, and grow
                by allocating (and freeing) heap storage as required.
            </p>
            <p class="calibre4">
                Goroutines are multiplexed onto multiple OS threads so if one should
                block, such as while waiting for I/O, others continue to run. Their
                design hides many of the complexities of thread creation and
                management.
            </p>
            <p class="calibre4">
                Prefix a function or method call with the <code class="calibre6">go</code>
                keyword to run the call in a new goroutine.
                When the call completes, the goroutine
                exits, silently. (The effect is similar to the Unix shell's
                <code class="calibre6">&amp;</code> notation for running a command in the
                background.)
            </p>
            <pre class="calibre7">go list.Sort()  // run list.Sort concurrently; don't wait for it.
</pre>
            <p class="calibre4">
                A function literal can be handy in a goroutine invocation.
            </p>
            <pre class="calibre7">func Announce(message string, delay time.Duration) {
    go func() {
        time.Sleep(delay)
        fmt.Println(message)
    }()  // Note the parentheses - must call the function.
}
</pre>
            <p class="calibre4">
                In Go, function literals are closures: the implementation makes
                sure the variables referred to by the function survive as long as they are active.
            </p>
            <p class="calibre4">
                These examples aren't too practical because the functions have no way of signaling
                completion. For that, we need channels.
            </p>

            <h3 id="channels" class="calibre5">Channels</h3>

            <p class="calibre4">
                Like maps, channels are allocated with <code class="calibre6">make</code>, and
                the resulting value acts as a reference to an underlying data structure.
                If an optional integer parameter is provided, it sets the buffer size for the channel.
                The default is zero, for an unbuffered or synchronous channel.
            </p>
            <pre class="calibre7">ci := make(chan int)            // unbuffered channel of integers
cj := make(chan int, 0)         // unbuffered channel of integers
cs := make(chan *os.File, 100)  // buffered channel of pointers to Files
</pre>
            <p class="calibre4">
                Unbuffered channels combine communication—the exchange of a value—with
                synchronization—guaranteeing that two calculations (goroutines) are in
                a known state.
            </p>
            <p class="calibre4">
                There are lots of nice idioms using channels. Here's one to get us started.
                In the previous section we launched a sort in the background. A channel
                can allow the launching goroutine to wait for the sort to complete.
            </p>
            <pre class="calibre7">c := make(chan int)  // Allocate a channel.
// Start the sort in a goroutine; when it completes, signal on the channel.
go func() {
    list.Sort()
    c &lt;- 1  // Send a signal; value does not matter.
}()
doSomethingForAWhile()
&lt;-c   // Wait for sort to finish; discard sent value.
</pre>
            <p class="calibre4">
                Receivers always block until there is data to receive.
                If the channel is unbuffered, the sender blocks until the receiver has
                received the value.
                If the channel has a buffer, the sender blocks only until the
                value has been copied to the buffer; if the buffer is full, this
                means waiting until some receiver has retrieved a value.
            </p>
            <p class="calibre4">
                A buffered channel can be used like a semaphore, for instance to
                limit throughput. In this example, incoming requests are passed
                to <code class="calibre6">handle</code>, which sends a value into the channel, processes
                the request, and then receives a value from the channel
                to ready the “semaphore” for the next consumer.
                The capacity of the channel buffer limits the number of
                simultaneous calls to <code class="calibre6">process</code>.
            </p>
            <pre class="calibre7">var sem = make(chan int, MaxOutstanding)

func handle(r *Request) {
    sem &lt;- 1    // Wait for active queue to drain.
    process(r)  // May take a long time.
    &lt;-sem       // Done; enable next request to run.
}

func Serve(queue chan *Request) {
    for {
        req := &lt;-queue
        go handle(req)  // Don't wait for handle to finish.
    }
}
</pre>

            <p class="calibre4">
                Once <code class="calibre6">MaxOutstanding</code> handlers are executing <code
                    class="calibre6">process</code>,
                any more will block trying to send into the filled channel buffer,
                until one of the existing handlers finishes and receives from the buffer.
            </p>

            <p class="calibre4">
                This design has a problem, though: <code class="calibre6">Serve</code>
                creates a new goroutine for
                every incoming request, even though only <code class="calibre6">MaxOutstanding</code>
                of them can run at any moment.
                As a result, the program can consume unlimited resources if the requests come in too fast.
                We can address that deficiency by changing <code class="calibre6">Serve</code> to
                gate the creation of the goroutines.
                Here's an obvious solution, but beware it has a bug we'll fix subsequently:
            </p>

            <pre class="calibre7">func Serve(queue chan *Request) {
    for req := range queue {
        sem &lt;- 1
        go func() {
            process(req) // Buggy; see explanation below.
            &lt;-sem
        }()
    }
}</pre>

            <p class="calibre4">
                The bug is that in a Go <code class="calibre6">for</code> loop, the loop variable
                is reused for each iteration, so the <code class="calibre6">req</code>
                variable is shared across all goroutines.
                That's not what we want.
                We need to make sure that <code class="calibre6">req</code> is unique for each goroutine.
                Here's one way to do that, passing the value of <code class="calibre6">req</code> as an argument
                to the closure in the goroutine:
            </p>

            <pre class="calibre7">func Serve(queue chan *Request) {
    for req := range queue {
        sem &lt;- 1
        go func(req *Request) {
            process(req)
            &lt;-sem
        }(req)
    }
}</pre>

            <p class="calibre4">
                Compare this version with the previous to see the difference in how
                the closure is declared and run.
                Another solution is just to create a new variable with the same
                name, as in this example:
            </p>

            <pre class="calibre7">func Serve(queue chan *Request) {
    for req := range queue {
        req := req // Create new instance of req for the goroutine.
        sem &lt;- 1
        go func() {
            process(req)
            &lt;-sem
        }()
    }
}</pre>

            <p class="calibre4">
                It may seem odd to write
            </p>

            <pre class="calibre7">req := req
</pre>

            <p class="calibre4">
                but it's legal and idiomatic in Go to do this.
                You get a fresh version of the variable with the same name, deliberately
                shadowing the loop variable locally but unique to each goroutine.
            </p>

            <p class="calibre4">
                Going back to the general problem of writing the server,
                another approach that manages resources well is to start a fixed
                number of <code class="calibre6">handle</code> goroutines all reading from the request
                channel.
                The number of goroutines limits the number of simultaneous
                calls to <code class="calibre6">process</code>.
                This <code class="calibre6">Serve</code> function also accepts a channel on which
                it will be told to exit; after launching the goroutines it blocks
                receiving from that channel.
            </p>

            <pre class="calibre7">func handle(queue chan *Request) {
    for r := range queue {
        process(r)
    }
}

func Serve(clientRequests chan *Request, quit chan bool) {
    // Start handlers
    for i := 0; i &lt; MaxOutstanding; i++ {
        go handle(clientRequests)
    }
    &lt;-quit  // Wait to be told to exit.
}
</pre>

            <h3 id="chan_of_chan" class="calibre5">Channels of channels</h3>
            <p class="calibre4">
                One of the most important properties of Go is that
                a channel is a first-class value that can be allocated and passed
                around like any other. A common use of this property is
                to implement safe, parallel demultiplexing.
            </p>
            <p class="calibre4">
                In the example in the previous section, <code class="calibre6">handle</code> was
                an idealized handler for a request but we didn't define the
                type it was handling. If that type includes a channel on which
                to reply, each client can provide its own path for the answer.
                Here's a schematic definition of type <code class="calibre6">Request</code>.
            </p>
            <pre class="calibre7">type Request struct {
    args        []int
    f           func([]int) int
    resultChan  chan int
}
</pre>
            <p class="calibre4">
                The client provides a function and its arguments, as well as
                a channel inside the request object on which to receive the answer.
            </p>
            <pre class="calibre7">func sum(a []int) (s int) {
    for _, v := range a {
        s += v
    }
    return
}

request := &amp;Request{[]int{3, 4, 5}, sum, make(chan int)}
// Send request
clientRequests &lt;- request
// Wait for response.
fmt.Printf("answer: %d\n", &lt;-request.resultChan)
</pre>
            <p class="calibre4">
                On the server side, the handler function is the only thing that changes.
            </p>
            <pre class="calibre7">func handle(queue chan *Request) {
    for req := range queue {
        req.resultChan &lt;- req.f(req.args)
    }
}
</pre>
            <p class="calibre4">
                There's clearly a lot more to do to make it realistic, but this
                code is a framework for a rate-limited, parallel, non-blocking RPC
                system, and there's not a mutex in sight.
            </p>

            <h3 id="parallel" class="calibre5">Parallelization</h3>
            <p class="calibre4">
                Another application of these ideas is to parallelize a calculation
                across multiple CPU cores. If the calculation can be broken into
                separate pieces that can execute independently, it can be parallelized,
                with a channel to signal when each piece completes.
            </p>
            <p class="calibre4">
                Let's say we have an expensive operation to perform on a vector of items,
                and that the value of the operation on each item is independent,
                as in this idealized example.
            </p>
            <pre class="calibre7">type Vector []float64

// Apply the operation to v[i], v[i+1] ... up to v[n-1].
func (v Vector) DoSome(i, n int, u Vector, c chan int) {
    for ; i &lt; n; i++ {
        v[i] += u.Op(v[i])
    }
    c &lt;- 1    // signal that this piece is done
}
</pre>
            <p class="calibre4">
                We launch the pieces independently in a loop, one per CPU.
                They can complete in any order but it doesn't matter; we just
                count the completion signals by draining the channel after
                launching all the goroutines.
            </p>
            <pre class="calibre7">const numCPU = 4 // number of CPU cores

func (v Vector) DoAll(u Vector) {
    c := make(chan int, numCPU)  // Buffering optional but sensible.
    for i := 0; i &lt; numCPU; i++ {
        go v.DoSome(i*len(v)/numCPU, (i+1)*len(v)/numCPU, u, c)
    }
    // Drain the channel.
    for i := 0; i &lt; numCPU; i++ {
        &lt;-c    // wait for one task to complete
    }
    // All done.
}
</pre>
            <p class="calibre4">
                Rather than create a constant value for numCPU, we can ask the runtime what
                value is appropriate.
                The function <code
                    class="calibre6"><a href="https://golang.org/pkg/runtime#NumCPU" class="pcalibre calibre">runtime.NumCPU</a></code>
                returns the number of hardware CPU cores in the machine, so we could write
            </p>
            <pre class="calibre7">var numCPU = runtime.NumCPU()
</pre>
            <p class="calibre4">
                There is also a function
                <code
                    class="calibre6"><a href="https://golang.org/pkg/runtime#GOMAXPROCS" class="pcalibre calibre">runtime.GOMAXPROCS</a></code>,
                which reports (or sets)
                the user-specified number of cores that a Go program can have running
                simultaneously.
                It defaults to the value of <code class="calibre6">runtime.NumCPU</code> but can be
                overridden by setting the similarly named shell environment variable
                or by calling the function with a positive number. Calling it with
                zero just queries the value.
                Therefore if we want to honor the user's resource request, we should write
            </p>
            <pre class="calibre7">var numCPU = runtime.GOMAXPROCS(0)
</pre>
            <p class="calibre4">
                Be sure not to confuse the ideas of concurrency—structuring a program
                as independently executing components—and parallelism—executing
                calculations in parallel for efficiency on multiple CPUs.
                Although the concurrency features of Go can make some problems easy
                to structure as parallel computations, Go is a concurrent language,
                not a parallel one, and not all parallelization problems fit Go's model.
                For a discussion of the distinction, see the talk cited in
                <a href="https://blog.golang.org/2013/01/concurrency-is-not-parallelism.html"
                    class="pcalibre calibre">this
                    blog post</a>.

            </p>
            <h3 id="leaky_buffer" class="calibre5">A leaky buffer</h3>

            <p class="calibre4">
                The tools of concurrent programming can even make non-concurrent
                ideas easier to express. Here's an example abstracted from an RPC
                package. The client goroutine loops receiving data from some source,
                perhaps a network. To avoid allocating and freeing buffers, it keeps
                a free list, and uses a buffered channel to represent it. If the
                channel is empty, a new buffer gets allocated.
                Once the message buffer is ready, it's sent to the server on
                <code class="calibre6">serverChan</code>.
            </p>
            <pre class="calibre7">var freeList = make(chan *Buffer, 100)
var serverChan = make(chan *Buffer)

func client() {
    for {
        var b *Buffer
        // Grab a buffer if available; allocate if not.
        select {
        case b = &lt;-freeList:
            // Got one; nothing more to do.
        default:
            // None free, so allocate a new one.
            b = new(Buffer)
        }
        load(b)              // Read next message from the net.
        serverChan &lt;- b      // Send to server.
    }
}
</pre>
            <p class="calibre4">
                The server loop receives each message from the client, processes it,
                and returns the buffer to the free list.
            </p>
            <pre class="calibre7">func server() {
    for {
        b := &lt;-serverChan    // Wait for work.
        process(b)
        // Reuse buffer if there's room.
        select {
        case freeList &lt;- b:
            // Buffer on free list; nothing more to do.
        default:
            // Free list full, just carry on.
        }
    }
}
</pre>
            <p class="calibre4">
                The client attempts to retrieve a buffer from <code class="calibre6">freeList</code>;
                if none is available, it allocates a fresh one.
                The server's send to <code class="calibre6">freeList</code> puts <code class="calibre6">b</code> back
                on the free list unless the list is full, in which case the
                buffer is dropped on the floor to be reclaimed by
                the garbage collector.
                (The <code class="calibre6">default</code> clauses in the <code class="calibre6">select</code>
                statements execute when no other case is ready,
                meaning that the <code class="calibre6">selects</code> never block.)
                This implementation builds a leaky bucket free list
                in just a few lines, relying on the buffered channel and
                the garbage collector for bookkeeping.
            </p>

        </div>
    </main>
</body>

</html>